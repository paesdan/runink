package dags

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"os"
	"path/filepath"
	"time"

	"github.com/runink/pkg/slice"
	"github.com/runink/pkg/raftstore"
	"github.com/runink/pkg/badgerstore"
	"github.com/runink/pkg/retrymanager"
	"github.com/runink/pkg/observability"
	"github.com/prometheus/client_golang/prometheus"
)

// Record defines the type flowing between steps
type Record struct {
	Data     map[string]interface{}
	Metadata map[string]string
	RunID    string
	SliceID  string
	StepName string
	Valid    bool
}

var (
	activeRuns = prometheus.NewGaugeVec(
		prometheus.GaugeOpts{
			Name: "runink_active_runs",
			Help: "Currently active pipeline runs.",
		},
		[]string{"feature", "scenario"},
	)

	successfulSlices = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "runink_successful_slices_total",
			Help: "Total number of successfully processed slices.",
		},
		[]string{"feature", "step"},
	)

	failedSlices = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "runink_failed_slices_total",
			Help: "Total number of failed slices.",
		},
		[]string{"feature", "step"},
	)
)

func init() {
	// Register metrics with Prometheus
	prometheus.MustRegister(activeRuns, successfulSlices, failedSlices)
}

func TradeCDMValidationPipeline(cfg *PipelineConfig) *slice.DAG {
	dag := slice.NewDAG(
		cfg.Feature,
		cfg.Scenario,
		slice.WithLayer(cfg.ModuleLayer),
		slice.WithWindow(cfg.Execution.Window),
		slice.WithMetadata(map[string]string{
			"contract_version":    cfg.Contract.Version,
			"contract_schema_hash": cfg.Contract.SchemaHash,
			"herd":                cfg.Herd.ID,
			"classification":      cfg.Herd.Labels.Classification,
			"compliance":          cfg.Herd.Labels.ComplianceCSV(),
			"slo_target":          cfg.Execution.SLOTarget,
			"lineage_tracking":    cfg.Herd.Labels.LineageTracking,
		}),
	)

	// Start ticker to trigger DAG window
	ticker := time.NewTicker(cfg.Execution.ParseWindowDuration())

	// Start ticker to prune old snapshots periodically
	prunerTicker := time.NewTicker(10 * time.Minute)

	go func() {
		for {
			select {
			case <-ticker.C:
				runID := observability.NewRunID()
				ctx := observability.ContextWithRunMetadata(runID)
				activeRuns.WithLabelValues(cfg.Feature, cfg.Scenario).Inc()

				sourceChan := make(chan *Record, 1000)
				decodeChan := make(chan *Record, 1000)
				validateChan := make(chan *Record, 1000)
				maskChan := make(chan *Record, 1000)
				tagChan := make(chan *Record, 1000)
				finalChan := make(chan *Record, 1000)

				go startSlice(ctx, cfg, "DecodeCDMEvent", sourceChan, decodeChan, DecodeCDMEvent)
				go startSlice(ctx, cfg, "ValidateMandatoryFields", decodeChan, validateChan, ValidateMandatoryFields)
				go startSlice(ctx, cfg, "ApplyFieldMasking", validateChan, maskChan, ApplyFieldMasking)
				go startSlice(ctx, cfg, "TagComplianceMetadata", maskChan, tagChan, TagComplianceMetadata)
				go startSlice(ctx, cfg, "DetectSchemaDrift", tagChan, finalChan, DetectSchemaDrift)

				go routeToSinks(ctx, finalChan, cfg)

				go ingestKafkaRecords(ctx, sourceChan)

			case <-prunerTicker.C:
				pruneOldSnapshots("/var/barn/backups", 7) // Keep last 7 snapshots
			}
		}
	}()

	return dag
}

//
// --- Helpers ---
//

// Ingest simulated Kafka records
func ingestKafkaRecords(ctx context.Context, outputChan chan<- *Record) {
	for i := 0; i < 100; i++ {
		rec := &Record{
			Data: map[string]interface{}{"trade_id": fmt.Sprintf("T%04d", i)},
			RunID: observability.GetRunID(ctx),
		}
		rec.SetRecordIDFromNaturalKey("trade_id")
		outputChan <- rec
	}
	close(outputChan)
}

// Start a slice step with retry, audit, checkpointing, and Prometheus instrumentation
func startSlice(ctx context.Context, cfg *PipelineConfig, stepName string, inputChan <-chan *Record, outputChan chan<- *Record, fn func(context.Context, *Record) (*Record, error)) {
	runID := observability.GetRunID(ctx)

	for rec := range inputChan {
		rec.StepName = stepName
		rec.SliceID = ComputeSliceID(runID, rec.RecordID(), stepName)

		var processed *Record
		err := retrymanager.Retry(func() error {
			var innerErr error
			processed, innerErr = fn(ctx, rec)
			return innerErr
		})

		if err != nil {
			observability.LogSliceFailure(ctx, rec, err)
			failedSlices.WithLabelValues(cfg.Feature, stepName).Inc()
			continue
		}

		if processed != nil {
			outputChan <- processed
			successfulSlices.WithLabelValues(cfg.Feature, stepName).Inc()

			if processed.Metadata != nil && processed.Metadata["checkpoint"] == "true" {
				hashedMeta := hashMetadata(processed.Metadata)
				raftstore.LogEvent(ctx, runID, stepName, hashedMeta)
				badgerstore.SaveKV(runID+"/"+processed.SliceID, hashedMeta)
			}
		}
	}
}

// Route records to sinks based on validity
func routeToSinks(ctx context.Context, finalChan <-chan *Record, cfg *PipelineConfig) {
	for rec := range finalChan {
		if rec.Valid {
			observability.EmitToSink(ctx, rec, cfg.Sinks.ValidSinkEnv)
		} else {
			observability.EmitToSink(ctx, rec, cfg.Sinks.InvalidSinkEnv)
		}
	}
}

// Compute deterministic SliceID = SHA256(RunID + RecordID + StepName)
func ComputeSliceID(runID, recordID, stepName string) string {
	h := sha256.New()
	h.Write([]byte(runID))
	h.Write([]byte(recordID))
	h.Write([]byte(stepName))
	return hex.EncodeToString(h.Sum(nil))
}

// Set RecordID from natural key (e.g., trade_id)
func (r *Record) SetRecordIDFromNaturalKey(key string) {
	if val, ok := r.Data[key]; ok {
		if r.Metadata == nil {
			r.Metadata = make(map[string]string)
		}
		r.Metadata["record_id"] = fmt.Sprintf("%v", val)
	}
}

// Get RecordID
func (r *Record) RecordID() string {
	if r.Metadata != nil {
		return r.Metadata["record_id"]
	}
	return observability.NewRandomID()
}

// Hash Metadata Map for secure logging/storage
func hashMetadata(meta map[string]string) map[string]string {
	hasher := sha256.New()
	for k, v := range meta {
		hasher.Write([]byte(k))
		hasher.Write([]byte(v))
	}
	return map[string]string{
		"metadata_hash": hex.EncodeToString(hasher.Sum(nil)),
	}
}

// Prune Old Snapshots from Btrfs backup directory (keep `retain` latest)
func pruneOldSnapshots(path string, retain int) {
	files, err := filepath.Glob(filepath.Join(path, "barn-final-*.bak"))
	if err != nil {
		return
	}
	if len(files) <= retain {
		return
	}

	// Sort files by oldest first
	sort.Strings(files)

	// Delete oldest
	for _, f := range files[:len(files)-retain] {
		_ = os.Remove(f)
	}
}
